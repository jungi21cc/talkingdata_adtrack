{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# 【If you run this script on the entire dataset within 16G RAM+64G swap ,then it will give LB:0.9798】\n",
    "# 【Any questions and how to run on entire dataset please feel free to contact me:QQ/Wechat:496852768,496852768@qq.com】\n",
    "\n",
    "# Original kernel:non-blending lightGBM model LB: 0.977:https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977?scriptVersionId=3224614\n",
    "# V0 Modified by Andy:Kaggle-runnable version of Baris Kanber's LightGBM:https://www.kaggle.com/aharless/kaggle-runnable-version-of-baris-kanber-s-lightgbm/comments\n",
    "# A non-blending lightGBM model that incorporates portions and ideas from various public kernels. \n",
    "\n",
    "# Possible improvement:\n",
    "# - Use lightgbm continue training to train full dataset?\n",
    "# - next_click?\n",
    "# - combine the advantages of former kernels?\n",
    "# - tune the parameters?\n",
    "# - consider using blend?\n",
    "# - Add new features on entire dataset one by one?\n",
    "# - delete all 'day' related features\n",
    "# - For my own validation (applying full predictions from script, shifted back by 1 day, to test set analogue)\n",
    "\n",
    "# V0: * Use functions to optimize the coding structure, useful to debug\n",
    "#     * Add many new group by features and 'click_time'\n",
    "#     3372.4 seconds,[127]\ttrain's auc: 0.987305\tvalid's auc: 0.972478, LB:0.9761, nchunk=25000000\n",
    "# V1: - Add all 'merge' function 'copy=False'【USEFUL for at least computational speed and prevent reach the RAM limit】\n",
    "#     Ran 2923 seconds, [129]\ttrain's auc: 0.987555\tvalid's auc: 0.972648, LB:0.9764\n",
    "# V2: - Add all 'astype' function 'copy=False'\n",
    "#     * Use default seed: 'data_random_seed','bagging_seed','feature_fraction_seed':1,2,3\n",
    "#     - Add lgb.plot_importance() for both 'split' and 'gain'\n",
    "#     - Reorganize the lgb training function\n",
    "#     * 2'next_click' features contains large randomness but EXTREMELY useful\n",
    "#     * delete 2'next_click' features, result same to the fork, [231]\ttrain's auc: 0.984031\tvalid's auc: 0.96488, LB:0.9661\n",
    "#     * then delete lowest score 'ip_app_channel_mean_hour' feature, [124]train's auc: 0.981878\tvalid's auc: 0.965349, LB:0.9662\n",
    "# V3: * 'nthread':for the best speed, set this to the number of real CPU cores, not the number of threads:http://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "#     - delete all the gc.collect() in the feature function\n",
    "#     * Different 'nthread' lead to different result. It is fixed to 4 after test.\n",
    "#     * Same result with along same code can have large gap in time, maybe due to the server disk, so time IS NOT fixed\n",
    "#     - Train on local computer, using GPU\n",
    "#     - Using new method to generate feature 'nextClick' rather than hash\n",
    "#     kaggle cpu:[151]\ttrain's auc: 0.988041\tvalid's auc: 0.973151,SAME to the fork, LB:0.9770, 3425.9s\n",
    "#     local cpu:[64]\ttrain's auc: 0.985661\tvalid's auc: 0.972455,SAME to the fork, LB:0.9761, 1625s\n",
    "#     local gpu:[105]\ttrain's auc: 0.987103\tvalid's auc: 0.97292，2235s\n",
    "#     local gpu double precesion:[64]\ttrain's auc: 0.985661\tvalid's auc: 0.972455, 1857s, SAME to local cpu\n",
    "# V4: - change nchunk from 25000000 to 75000000\n",
    "#     CPU:[326]\ttrain's auc: 0.985568\tvalid's auc: 0.990381，LB:0.9794, use 30G swap, almost 5hour\n",
    "#     - change nchunk from 75000000 to 140000000\n",
    "#     - delete 'day'.'day'只是用来切块每一天的某小时(区分不同天)，本身不用作分类\n",
    "#     - delete 'ip_tchan_count','ip_app_channel_mean_hour','ip_app_os_var','X7' temporarily for speed\n",
    "#     - [162]train's auc: 0.984942\tvalid's auc: 0.99025, 19709s, LB:0.9791\n",
    "# V5: - add 'ip_tchan_count','ip_app_channel_mean_hour','ip_app_os_var','X7' back(IMPORTANT)\n",
    "#     - change nchunk from 140000000 to 150000000\n",
    "#     [327]\ttrain's auc: 0.985816\tvalid's auc: 0.990797\n",
    "#     - add 'day' for test\n",
    "#     - early_stopping_rounds=30 to 50\n",
    "#     [650]\ttrain's auc: 0.986541\tvalid's auc: 0.99073, 16G RAM+62G swap, 28562s, LB:0.9796\n",
    "# V6: - delete 'day'\n",
    "#     - change nchunk from 150000000 to all\n",
    "#     [309]\ttrain's auc: 0.985642\tvalid's auc: 0.990719, 16G RAM+40G swap, 23376s, LB:0.9798\n",
    "#     [15766.776413679123]: model training time\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_count( df, group_cols, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Aggregating by \", group_cols , '...' )\n",
    "    gp = df[group_cols][group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_countuniq( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Counting unqiue \", counted, \" by \", group_cols , '...' )\n",
    "    # print('the Id of train_df while function before merge: ',id(df)) # the same with train_df\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    # print('the Id of train_df while function after merge: ',id(df)) # id changes\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cumcount( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Cumulative count by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount()\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mean( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating mean of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_var( df, group_cols, counted, agg_name, agg_type='float32', show_max=False, show_agg=True ):\n",
    "    if show_agg:\n",
    "        print( \"Calculating variance of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(dtrain, dvalid, predictors, target='target', feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None,metrics='auc'):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'learning_rate': 0.2, # 【consider using 0.1】\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'scale_pos_weight': 200, # because training data is extremely unbalanced\n",
    "        'num_leaves': 7,  # we should let it be smaller than 2^(max_depth), default=31\n",
    "        'max_depth': 3,  # -1 means no limit, default=-1\n",
    "        'min_data_per_leaf': 100,  # alias=min_data_per_leaf , min_data, min_child_samples, default=20\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values,default=255\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.default=1.0, alias=bagging_fraction\n",
    "        'subsample_freq': 1,  # k means will perform bagging at every k iteration, <=0 means no enable,alias=bagging_freq,default=0\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.alias:feature_fraction\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf),default=1e-3,Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4, # should be equal to REAL cores:http://xgboost.readthedocs.io/en/latest/how_to/external_memory.html\n",
    "        'verbose': 0\n",
    "        #         'device': 'gpu',\n",
    "#         'gpu_platform_id':1\n",
    "        # gpu_use_dp, default=false,set to true to use double precision math on GPU (default using single precision)\n",
    "#         'gpu_device_id': 2 #default=-1,default value is -1, means the default device in the selected platform\n",
    "        # 'random_state':666 [LightGBM] [Warning] Unknown parameter: random_state\n",
    "        # 'feature_fraction_seed': 666,\n",
    "        # 'bagging_seed': 666, # alias=bagging_fraction_seed\n",
    "        # 'data_random_seed': 666 # random seed for data partition in parallel learning (not include feature parallel)\n",
    "    }\n",
    "    # lgb_params.update(params) # Python dict.update()\n",
    "\n",
    "    print(\"load train_df into lgb.Dataset...\")\n",
    "    # free_raw_data (bool, optional (default=True)) – If True, raw data is freed after constructing inner Dataset.\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dtrain\n",
    "    print(\"load valid_df into lgb.Dataset...\")\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "    \n",
    "    # Warning:basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
    "    # https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L679\n",
    "    # https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L483\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "    \n",
    "    del xgtrain, xgvalid\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "    gc.collect()\n",
    "\n",
    "    return (bst1,bst1.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "def DO(frm,to,fileno):\n",
    "    dtypes = {\n",
    "            'ip'            : 'uint32',\n",
    "            'app'           : 'uint16',\n",
    "            'device'        : 'uint16',\n",
    "            'os'            : 'uint16',\n",
    "            'channel'       : 'uint16',\n",
    "            'is_attributed' : 'uint8', # 【consider bool?need test】\n",
    "            'click_id'      : 'uint32', # 【consider 'attributed_time'?】\n",
    "            }\n",
    "    \n",
    "    print('loading train data...',frm,to)\n",
    "    # usecols:Using this parameter results in much faster parsing time and lower memory usage.\n",
    "    train_df = pd.read_csv(\"input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "    print('loading test data...')\n",
    "    if debug:\n",
    "        test_df = pd.read_csv(\"input/test.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"input/test.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "    len_train = len(train_df)\n",
    "    train_df=train_df.append(test_df) # Shouldn't process individually,because of lots of count,mean,var variables\n",
    "    # train_df['is_attributed'] = train_df['is_attributed'].fillna(-1)\n",
    "    train_df['is_attributed'].fillna(-1,inplace=True)\n",
    "    train_df['is_attributed'] = train_df['is_attributed'].astype('uint8',copy=False)\n",
    "    # train_df['click_id'] = train_df['click_id'].fillna(-1)\n",
    "    train_df['click_id'].fillna(-1,inplace=True)\n",
    "    train_df['click_id'] = train_df['click_id'].astype('uint32',copy=False)\n",
    "    \n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Extracting new features...')\n",
    "    train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "    train_df['day'] = pd.to_datetime(train_df.click_time).dt.day.astype('uint8')\n",
    "    gc.collect()\n",
    "    \n",
    "    # print('the Id of train_df before function: ',id(train_df))\n",
    "    train_df = do_countuniq( train_df, ['ip'], 'channel', 'X0', 'uint8', show_max=False ); gc.collect()\n",
    "    # print('the Id of train_df after function: ',id(train_df)) # the same id with 'df' returned\n",
    "    train_df = do_countuniq( train_df, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['ip'], 'app', 'X3', 'uint16', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['ip'], 'device', 'X5', 'uint16', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['app'], 'channel', 'X6','uint8', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'app', 'X8','uint8', show_max=False ); gc.collect()\n",
    "    train_df = do_cumcount( train_df, ['ip'], 'os', 'X7', show_max=False ); gc.collect()\n",
    "    train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'app', 'X1', show_max=False ); gc.collect()\n",
    "    train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'channel', 'A0', show_max=False ); gc.collect()\n",
    "    train_df = do_count( train_df, ['ip', 'app', 'channel'], 'A1', show_max=False ); gc.collect()\n",
    "    train_df = do_count( train_df, ['ip', 'device', 'os','app'], 'A2', show_max=False ); gc.collect()\n",
    "    # ip-device-hour?\n",
    "\n",
    "    train_df = do_count( train_df, ['ip', 'day', 'hour'], 'ip_tcount','uint16',show_max=False ); gc.collect()\n",
    "#     train_df = do_count( train_df, ['ip', 'hour'], 'ip_tcount2','uint32',show_max=False ); gc.collect()\n",
    "    train_df = do_count( train_df, ['ip', 'app'], 'ip_app_count','uint32', show_max=False ); gc.collect()\n",
    "    train_df = do_count( train_df, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=False ); gc.collect()\n",
    "    train_df = do_var( train_df, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=False ); gc.collect()\n",
    "    train_df = do_var( train_df, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=False ); gc.collect()\n",
    "    train_df = do_var( train_df, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=False ); gc.collect()\n",
    "    train_df = do_mean( train_df, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=False ); gc.collect()\n",
    "\n",
    "\n",
    "# nextclick----------------------------------------------------------------------------------------------------------\n",
    "    # print('doing nextClick')\n",
    "    # predictors=[]\n",
    "    # new_feature = 'nextClick'\n",
    "    # filename='nextClick_%d_%d.csv'%(frm,to)\n",
    "\n",
    "    # if os.path.exists(filename):\n",
    "    #     print('loading from save file')\n",
    "    #     QQ=pd.read_csv(filename).values\n",
    "    # else:\n",
    "    #     D=2**26\n",
    "    #     train_df['category'] = (train_df['ip'].astype(str) + \"_\" + train_df['app'].astype(str) + \"_\" + train_df['device'].astype(str) \\\n",
    "    #         + \"_\" + train_df['os'].astype(str)).apply(hash) % D\n",
    "    #     # from 1970/1/1, 50year*365day*24*60*60=1,576,800,000 seconds, so 2,000,000,000 is enough\n",
    "    #     click_buffer= np.full(D, 3000000000, dtype=np.uint32) # Return a new array of given shape and type, filled with fill_value.\n",
    "        \n",
    "    #     train_df['epochtime']= train_df['click_time'].astype(np.int64,copy=False) // 10 ** 9\n",
    "    #     next_clicks= []\n",
    "    #     # After reverse, the time becomes future to past, make next_clicks positive\n",
    "    #     for category, t in zip(reversed(train_df['category'].values), reversed(train_df['epochtime'].values)):\n",
    "    #         next_clicks.append(click_buffer[category]-t)\n",
    "    #         click_buffer[category]= t\n",
    "    #     del(click_buffer)\n",
    "    #     QQ= list(reversed(next_clicks))\n",
    "\n",
    "    #     if not debug:\n",
    "    #         print('saving')\n",
    "    #         pd.DataFrame(QQ).to_csv(filename,index=False)\n",
    "            \n",
    "    # train_df.drop(['epochtime','category','click_time'], axis=1, inplace=True)\n",
    "\n",
    "    # train_df[new_feature] = pd.Series(QQ).astype('float32',copy=False)\n",
    "    # predictors.append(new_feature)\n",
    "    # train_df[new_feature+'_shift'] = train_df[new_feature].shift(+1).values\n",
    "    # predictors.append(new_feature+'_shift')\n",
    "    \n",
    "    # del QQ\n",
    "    # gc.collect()\n",
    "    \n",
    "#=====================================================================================================\n",
    "    print('doing nextClick 2...')\n",
    "    predictors=[]\n",
    "    \n",
    "    train_df['click_time'] = (train_df['click_time'].astype(np.int64,copy=False) // 10 ** 9).astype(np.int32,copy=False)\n",
    "    train_df['nextClick'] = (train_df.groupby(['ip', 'app', 'device', 'os']).click_time.shift(-1) - train_df.click_time).astype(np.float32,copy=False)\n",
    "    print(train_df['nextClick'].head(30))\n",
    "    train_df.drop(['click_time','day'], axis=1, inplace=True)\n",
    "    predictors.append('nextClick')\n",
    "    gc.collect()\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "    print(\"vars and data type: \")\n",
    "    target = 'is_attributed'\n",
    "    predictors.extend(['app','device','os', 'channel', 'hour',\n",
    "                  'ip_tcount', 'ip_tchan_count', 'ip_app_count',\n",
    "                  'ip_app_os_count', 'ip_app_os_var',\n",
    "                  'ip_app_channel_var_day','ip_app_channel_mean_hour',\n",
    "                  'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8'])\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour',]\n",
    "    print('predictors',predictors)\n",
    "\n",
    "    test_df = train_df[len_train:]\n",
    "    test_df.drop(columns='is_attributed',inplace=True)\n",
    "    train_df.drop(columns='click_id',inplace=True)\n",
    "    val_df = train_df[(len_train-val_size):len_train] # Validation set\n",
    "    train_df = train_df[:(len_train-val_size)]\n",
    "    \n",
    "    print(\"train size: \", len(train_df))\n",
    "    print(\"valid size: \", len(val_df))\n",
    "    print(\"test size : \", len(test_df))\n",
    "    train_df.info()\n",
    "\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id']\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(\n",
    "                            train_df, \n",
    "                            val_df, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            early_stopping_rounds=50, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=2000, \n",
    "                            categorical_features=categorical)\n",
    "    del train_df\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "\n",
    "    print('Plot feature importances...')\n",
    "    lgb.plot_importance(bst)\n",
    "    # plt.show()\n",
    "    plt.gcf().savefig('feature_importance_runnablelightgbm_split.png')\n",
    "    lgb.plot_importance(bst,importance_type='gain')\n",
    "    plt.gcf().savefig('feature_importance_runnablelightgbm_gain.png')\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    sub['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    del test_df\n",
    "    if not debug:\n",
    "        print(\"writing...\")\n",
    "        sub.to_csv('sub_it%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "    del sub\n",
    "    gc.collect()\n",
    "    print(\"All done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function-------------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    inpath = 'input/'\n",
    "    \n",
    "    #【In order to get 0.9798, you have to change nchunk to all and frm to 0 to use entire dataset】\n",
    "    nrows=184903891-1 # the first line is columns' name\n",
    "    nchunk=25000000 # 【The more the better】\n",
    "    val_size=2500000\n",
    "    frm=nrows-75000000\n",
    "    \n",
    "    debug=False\n",
    "    debug=True\n",
    "    if debug:\n",
    "        print('*** Debug: this is a test run for debugging purposes ***')\n",
    "        frm=0\n",
    "        nchunk=100000\n",
    "        val_size=10000\n",
    "    \n",
    "    to=frm+nchunk\n",
    "    \n",
    "    DO(frm,to,6) # fileno start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
